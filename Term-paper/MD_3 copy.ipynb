{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9d69947",
   "metadata": {},
   "source": [
    "# Read and merge all CSV files from `data` (only import pandas)\n",
    "\n",
    "This notebook reads every CSV in `Term-paper/data` using only `pandas` as an explicit import,\n",
    "lists files using the Jupyter/IPython shell, then merges them into a single DataFrame.\n",
    "If DataFrames share column names the merge uses those columns; otherwise DataFrames are concatenated side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7e5fa300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ec4f6567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 140 CSV files\n",
      "Skipped CPI.csv (no userid column)\n",
      "Loaded SCE-Apr-2014.csv ((1311, 30)), has userid\n",
      "Loaded SCE-Apr-2015.csv ((1283, 30)), has userid\n",
      "Loaded SCE-Apr-2016.csv ((1214, 30)), has userid\n",
      "Loaded SCE-Apr-2017.csv ((1276, 30)), has userid\n",
      "Loaded SCE-Apr-2018.csv ((1300, 30)), has userid\n",
      "Loaded SCE-Apr-2019.csv ((1336, 30)), has userid\n",
      "Loaded SCE-Apr-2020.csv ((1300, 30)), has userid\n",
      "Loaded SCE-Apr-2021.csv ((1243, 30)), has userid\n",
      "Loaded SCE-Apr-2022.csv ((1269, 30)), has userid\n",
      "Loaded SCE-Apr-2023.csv ((1255, 30)), has userid\n",
      "Loaded SCE-Apr-2024.csv ((1082, 30)), has userid\n",
      "Loaded SCE-Aug-2013.csv ((1769, 30)), has userid\n",
      "Loaded SCE-Aug-2014.csv ((1352, 30)), has userid\n",
      "Loaded SCE-Aug-2015.csv ((1226, 30)), has userid\n",
      "Loaded SCE-Aug-2016.csv ((1271, 30)), has userid\n",
      "Loaded SCE-Aug-2017.csv ((1344, 30)), has userid\n",
      "Loaded SCE-Aug-2018.csv ((1331, 30)), has userid\n",
      "Loaded SCE-Aug-2019.csv ((1290, 30)), has userid\n",
      "Loaded SCE-Aug-2020.csv ((1193, 30)), has userid\n",
      "Loaded SCE-Aug-2021.csv ((1265, 30)), has userid\n",
      "Loaded SCE-Aug-2022.csv ((1300, 30)), has userid\n",
      "Loaded SCE-Aug-2023.csv ((1130, 30)), has userid\n",
      "Loaded SCE-Aug-2024.csv ((1108, 30)), has userid\n",
      "Loaded SCE-Dec-2013.csv ((1350, 30)), has userid\n",
      "Loaded SCE-Dec-2014.csv ((1302, 30)), has userid\n",
      "Loaded SCE-Dec-2015.csv ((1201, 30)), has userid\n",
      "Loaded SCE-Dec-2016.csv ((1359, 30)), has userid\n",
      "Loaded SCE-Dec-2017.csv ((1273, 30)), has userid\n",
      "Loaded SCE-Dec-2018.csv ((1268, 30)), has userid\n",
      "Loaded SCE-Dec-2019.csv ((1262, 30)), has userid\n",
      "Loaded SCE-Dec-2020.csv ((1337, 30)), has userid\n",
      "Loaded SCE-Dec-2021.csv ((1283, 30)), has userid\n",
      "Loaded SCE-Dec-2022.csv ((1158, 30)), has userid\n",
      "Loaded SCE-Dec-2023.csv ((1120, 30)), has userid\n",
      "Loaded SCE-Dec-2024.csv ((976, 30)), has userid\n",
      "Loaded SCE-Feb-2014.csv ((1295, 30)), has userid\n",
      "Loaded SCE-Feb-2015.csv ((1314, 30)), has userid\n",
      "Loaded SCE-Feb-2016.csv ((1178, 30)), has userid\n",
      "Loaded SCE-Feb-2017.csv ((1341, 30)), has userid\n",
      "Loaded SCE-Feb-2018.csv ((1322, 30)), has userid\n",
      "Loaded SCE-Feb-2019.csv ((1359, 30)), has userid\n",
      "Loaded SCE-Feb-2020.csv ((1330, 30)), has userid\n",
      "Loaded SCE-Feb-2021.csv ((1243, 30)), has userid\n",
      "Loaded SCE-Feb-2022.csv ((1210, 30)), has userid\n",
      "Loaded SCE-Feb-2023.csv ((1208, 30)), has userid\n",
      "Loaded SCE-Feb-2024.csv ((1129, 30)), has userid\n",
      "Loaded SCE-Jan-2014.csv ((1328, 30)), has userid\n",
      "Loaded SCE-Jan-2015.csv ((1308, 30)), has userid\n",
      "Loaded SCE-Jan-2016.csv ((1232, 30)), has userid\n",
      "Loaded SCE-Jan-2017.csv ((1358, 30)), has userid\n",
      "Loaded SCE-Jan-2018.csv ((1323, 30)), has userid\n",
      "Loaded SCE-Jan-2019.csv ((1372, 30)), has userid\n",
      "Loaded SCE-Jan-2020.csv ((1317, 30)), has userid\n",
      "Loaded SCE-Jan-2021.csv ((1259, 30)), has userid\n",
      "Loaded SCE-Jan-2022.csv ((1235, 30)), has userid\n",
      "Loaded SCE-Jan-2023.csv ((1178, 30)), has userid\n",
      "Loaded SCE-Jan-2024.csv ((1128, 30)), has userid\n",
      "Loaded SCE-Jul-2013.csv ((1197, 30)), has userid\n",
      "Loaded SCE-Jul-2014.csv ((1373, 30)), has userid\n",
      "Loaded SCE-Jul-2015.csv ((1256, 30)), has userid\n",
      "Loaded SCE-Jul-2016.csv ((1305, 30)), has userid\n",
      "Loaded SCE-Jul-2017.csv ((1354, 30)), has userid\n",
      "Loaded SCE-Jul-2018.csv ((1330, 30)), has userid\n",
      "Loaded SCE-Jul-2019.csv ((1318, 30)), has userid\n",
      "Loaded SCE-Jul-2020.csv ((1205, 30)), has userid\n",
      "Loaded SCE-Jul-2021.csv ((1239, 30)), has userid\n",
      "Loaded SCE-Jul-2022.csv ((1305, 30)), has userid\n",
      "Loaded SCE-Jul-2023.csv ((1130, 30)), has userid\n",
      "Loaded SCE-Jul-2024.csv ((1099, 30)), has userid\n",
      "Loaded SCE-Jun-2013.csv ((1253, 30)), has userid\n",
      "Loaded SCE-Jun-2014.csv ((1302, 30)), has userid\n",
      "Loaded SCE-Jun-2015.csv ((1271, 30)), has userid\n",
      "Loaded SCE-Jun-2016.csv ((1261, 30)), has userid\n",
      "Loaded SCE-Jun-2017.csv ((1349, 30)), has userid\n",
      "Loaded SCE-Jun-2018.csv ((1306, 30)), has userid\n",
      "Loaded SCE-Jun-2019.csv ((1330, 30)), has userid\n",
      "Loaded SCE-Jun-2020.csv ((1200, 30)), has userid\n",
      "Loaded SCE-Jun-2021.csv ((1297, 30)), has userid\n",
      "Loaded SCE-Jun-2022.csv ((1217, 30)), has userid\n",
      "Loaded SCE-Jun-2023.csv ((1159, 30)), has userid\n",
      "Loaded SCE-Jun-2024.csv ((1086, 30)), has userid\n",
      "Loaded SCE-Mar-2014.csv ((1309, 30)), has userid\n",
      "Loaded SCE-Mar-2015.csv ((1288, 30)), has userid\n",
      "Loaded SCE-Mar-2016.csv ((1242, 30)), has userid\n",
      "Loaded SCE-Mar-2017.csv ((1365, 30)), has userid\n",
      "Loaded SCE-Mar-2018.csv ((1315, 30)), has userid\n",
      "Loaded SCE-Mar-2019.csv ((1368, 30)), has userid\n",
      "Loaded SCE-Mar-2020.csv ((1300, 30)), has userid\n",
      "Loaded SCE-Mar-2021.csv ((1221, 30)), has userid\n",
      "Loaded SCE-Mar-2022.csv ((1273, 30)), has userid\n",
      "Loaded SCE-Mar-2023.csv ((1254, 30)), has userid\n",
      "Loaded SCE-Mar-2024.csv ((1120, 30)), has userid\n",
      "Loaded SCE-May-2014.csv ((1280, 30)), has userid\n",
      "Loaded SCE-May-2015.csv ((1276, 30)), has userid\n",
      "Loaded SCE-May-2016.csv ((1259, 30)), has userid\n",
      "Loaded SCE-May-2017.csv ((1349, 30)), has userid\n",
      "Loaded SCE-May-2018.csv ((1312, 30)), has userid\n",
      "Loaded SCE-May-2019.csv ((1321, 30)), has userid\n",
      "Loaded SCE-May-2020.csv ((1266, 30)), has userid\n",
      "Loaded SCE-May-2021.csv ((1234, 30)), has userid\n",
      "Loaded SCE-May-2022.csv ((1215, 30)), has userid\n",
      "Loaded SCE-May-2023.csv ((1215, 30)), has userid\n",
      "Loaded SCE-May-2024.csv ((1092, 30)), has userid\n",
      "Loaded SCE-Nov-2013.csv ((1541, 30)), has userid\n",
      "Loaded SCE-Nov-2014.csv ((1312, 30)), has userid\n",
      "Loaded SCE-Nov-2015.csv ((1242, 30)), has userid\n",
      "Loaded SCE-Nov-2016.csv ((1321, 30)), has userid\n",
      "Loaded SCE-Nov-2017.csv ((1337, 30)), has userid\n",
      "Loaded SCE-Nov-2018.csv ((1323, 30)), has userid\n",
      "Loaded SCE-Nov-2019.csv ((1283, 30)), has userid\n",
      "Loaded SCE-Nov-2020.csv ((1233, 30)), has userid\n",
      "Loaded SCE-Nov-2021.csv ((1281, 30)), has userid\n",
      "Loaded SCE-Nov-2022.csv ((1184, 30)), has userid\n",
      "Loaded SCE-Nov-2023.csv ((1098, 30)), has userid\n",
      "Loaded SCE-Nov-2024.csv ((1037, 30)), has userid\n",
      "Loaded SCE-Oct-2013.csv ((1538, 30)), has userid\n",
      "Loaded SCE-Oct-2014.csv ((1328, 30)), has userid\n",
      "Loaded SCE-Oct-2015.csv ((1248, 30)), has userid\n",
      "Loaded SCE-Oct-2016.csv ((1319, 30)), has userid\n",
      "Loaded SCE-Oct-2017.csv ((1377, 30)), has userid\n",
      "Loaded SCE-Oct-2018.csv ((1354, 30)), has userid\n",
      "Loaded SCE-Oct-2019.csv ((1309, 30)), has userid\n",
      "Loaded SCE-Oct-2020.csv ((1197, 30)), has userid\n",
      "Loaded SCE-Oct-2021.csv ((1283, 30)), has userid\n",
      "Loaded SCE-Oct-2022.csv ((1181, 30)), has userid\n",
      "Loaded SCE-Oct-2023.csv ((1124, 30)), has userid\n",
      "Loaded SCE-Oct-2024.csv ((1076, 30)), has userid\n",
      "Loaded SCE-Sep-2013.csv ((1529, 30)), has userid\n",
      "Loaded SCE-Sep-2014.csv ((1320, 30)), has userid\n",
      "Loaded SCE-Sep-2015.csv ((1262, 30)), has userid\n",
      "Loaded SCE-Sep-2016.csv ((1319, 30)), has userid\n",
      "Loaded SCE-Sep-2017.csv ((1325, 30)), has userid\n",
      "Loaded SCE-Sep-2018.csv ((1302, 30)), has userid\n",
      "Loaded SCE-Sep-2019.csv ((1299, 30)), has userid\n",
      "Loaded SCE-Sep-2020.csv ((1166, 30)), has userid\n",
      "Loaded SCE-Sep-2021.csv ((1261, 30)), has userid\n",
      "Loaded SCE-Sep-2022.csv ((1271, 30)), has userid\n",
      "Loaded SCE-Sep-2023.csv ((1112, 30)), has userid\n",
      "Loaded SCE-Sep-2024.csv ((1089, 30)), has userid\n",
      "Final concatenated shape: (176101, 30)\n"
     ]
    }
   ],
   "source": [
    "#1.1\n",
    "\n",
    "data_path = 'data'\n",
    "\n",
    "#  Step 1: Collect CSV files \n",
    "csv_files = [f for f in os.listdir(data_path) if f.lower().endswith('.csv')]\n",
    "print(f\"Found {len(csv_files)} CSV files\")\n",
    "\n",
    "dfs = []\n",
    "\n",
    "#  Step 2: Load only datasets that have userid \n",
    "for fname in csv_files:\n",
    "    full_path = os.path.join(data_path, fname)\n",
    "    try:\n",
    "        df = pd.read_csv(full_path, sep=';')\n",
    "        if 'userid' in df.columns:\n",
    "            df['source_file'] = fname  # tag for later identification\n",
    "            dfs.append(df)\n",
    "            print(f\"Loaded {fname} ({df.shape}), has userid\")\n",
    "        else:\n",
    "            print(f\"Skipped {fname} (no userid column)\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to read {fname}: {e}\")\n",
    "\n",
    "#  Step 3: Concatenate vertically (merging vertically) \n",
    "if dfs:\n",
    "    merged_df = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"Final concatenated shape: {merged_df.shape}\")\n",
    "\n",
    "\n",
    "#  Step 4: Drop any unwanted column safely \n",
    "for col in merged_df.columns:\n",
    "    if 'DATE,CPI' in col:\n",
    "        merged_df = merged_df.drop(columns=[col])\n",
    "        print(f\"Dropped column: {col}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "df272aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique user IDs: 23369\n"
     ]
    }
   ],
   "source": [
    "#1.2.1\n",
    "print(\"Number of unique user IDs:\", merged_df['userid'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "bbfb76eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 176101\n"
     ]
    }
   ],
   "source": [
    "#1.2.2\n",
    "print(\"Total rows:\", len(merged_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ba518868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique survey waves: 139\n"
     ]
    }
   ],
   "source": [
    "#1.2.3\n",
    "print(\"Number of unique survey waves:\", merged_df['source_file'].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7dcabca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Earliest date: 2013-06-01 00:00:00\n",
      "Latest date: 2024-12-01 00:00:00\n"
     ]
    }
   ],
   "source": [
    "#1.2.4\n",
    "\n",
    "# Extract month and year separately\n",
    "merged_df[['month_str', 'year_str']] = merged_df['source_file'].str.extract(r'SCE-([A-Za-z]+)-(\\d{4})')\n",
    "\n",
    "# Combine and convert to datetime\n",
    "merged_df['survey_date'] = pd.to_datetime(\n",
    "    merged_df['month_str'] + ' ' + merged_df['year_str'],\n",
    "    format='%b %Y',\n",
    "    errors='coerce'  # ignore weird filenames\n",
    ")\n",
    "\n",
    "# Sanity check\n",
    "print(\"Earliest date:\", merged_df['survey_date'].min())\n",
    "print(\"Latest date:\", merged_df['survey_date'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "48871d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeracy columns: ['num_lit_q1_correct', 'num_lit_q2_correct', 'num_lit_q3_correct', 'num_lit_q5_correct', 'num_lit_q6_correct', 'num_lit_q8_correct', 'num_lit_q9_correct']\n",
      "Rows with missing numeracy values after fill: 37378\n",
      "Users with all numeracy missing after fill: 65\n"
     ]
    }
   ],
   "source": [
    "#2.1\n",
    "\n",
    "# Identify numeracy columns\n",
    "num_lit_cols = [\n",
    "    col for col in merged_df.columns\n",
    "    if \"num_lit_q\" in col and \"correct\" in col\n",
    "]\n",
    "print(\"Numeracy columns:\", num_lit_cols)\n",
    "\n",
    "# Clean non-standard missing markers\n",
    "merged_df[num_lit_cols] = merged_df[num_lit_cols].replace(\n",
    "    ['', 'NA', 'N/A', 'null', 'None', 'nan', -99, 999],\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "# Sort so the first survey per user is first\n",
    "merged_df = merged_df.sort_values(['userid', 'survey_date'])\n",
    "\n",
    "# Extract each user’s first valid numeracy responses\n",
    "first_vals = (\n",
    "    merged_df.groupby('userid', sort=False)[num_lit_cols]\n",
    "    .first()\n",
    "    .add_suffix('_first')\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Merge those first-wave values back\n",
    "merged_df = merged_df.merge(first_vals, on='userid', how='left')\n",
    "\n",
    "# Fill missing numeracy values from the user’s first record\n",
    "for col in num_lit_cols:\n",
    "    merged_df[col] = merged_df[col].fillna(merged_df[f\"{col}_first\"])\n",
    "    merged_df.drop(columns=f\"{col}_first\", inplace=True)\n",
    "\n",
    "# Verification (fully vectorized)\n",
    "rows_missing_after = merged_df[num_lit_cols].isnull().any(axis=1).sum()\n",
    "\n",
    "# Users with all numeracy missing\n",
    "users_all_missing_mask = merged_df[num_lit_cols].isnull().all(axis=1)\n",
    "users_all_missing = merged_df.loc[users_all_missing_mask, 'userid'].nunique()\n",
    "\n",
    "print(f\"Rows with missing numeracy values after fill: {rows_missing_after}\")\n",
    "print(f\"Users with all numeracy missing after fill: {users_all_missing}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1026af1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After dropping missing ['female', 'age', 'educ']: 175233 (Dropped 868)\n",
      "After dropping missing ['inflation', 'house_price_change', 'prob_stocks_up']: 173550 (Dropped 1683)\n",
      "After dropping missing numeracy columns: 137576 (Dropped 35974)\n"
     ]
    }
   ],
   "source": [
    "#2.2\n",
    "\n",
    "# Step 1: Drop rows with missing demographics \n",
    "demographic_cols = ['female', 'age', 'educ']\n",
    "merged_demo = merged_df.dropna(subset=demographic_cols)\n",
    "after_demo = len(merged_demo)\n",
    "print(f\"After dropping missing {demographic_cols}: {after_demo} (Dropped {before_drop - after_demo})\")\n",
    "\n",
    "# Step 2: Drop rows with missing expectations \n",
    "expectation_cols = ['inflation', 'house_price_change', 'prob_stocks_up']\n",
    "merged_expect = merged_demo.dropna(subset=expectation_cols)\n",
    "after_expect = len(merged_expect)\n",
    "print(f\"After dropping missing {expectation_cols}: {after_expect} (Dropped {after_demo - after_expect})\")\n",
    "\n",
    "# Step 3: Drop rows with missing numeracy questions \n",
    "merged_final = merged_expect.dropna(subset=num_lit_cols)\n",
    "after_num = len(merged_final)\n",
    "print(f\"After dropping missing numeracy columns: {after_num} (Dropped {after_expect - after_num})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eef42a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inflation: Dropped 139 below -50.00 (min = -60.0)\n",
      "house_price_change: Dropped 162 below -35.00 (min = -45.0)\n",
      "prob_stocks_up: Dropped 0 below 0.00 (min = 0.0)\n",
      "\n",
      "Total observations dropped below 0.1th percentile: 301\n",
      "Remaining dataset shape: (173446, 33)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#2.3.1\n",
    "\n",
    "expectation_cols = ['inflation', 'house_price_change', 'prob_stocks_up']\n",
    "expectation_cols = [c for c in expectation_cols if c in merged_df.columns]\n",
    "\n",
    "total_below = 0\n",
    "\n",
    "for col in expectation_cols:\n",
    "    merged_df[col] = pd.to_numeric(merged_df[col], errors='coerce')\n",
    "    q_low = merged_df[col].quantile(0.001)\n",
    "    min_val = merged_df[col].min()\n",
    "\n",
    "    n_below = (merged_df[col] < q_low).sum()\n",
    "    merged_df = merged_df[merged_df[col] >= q_low]\n",
    "\n",
    "    total_below += n_below\n",
    "    print(f\"{col}: Dropped {n_below} below {q_low:.2f} (min = {min_val})\")\n",
    "\n",
    "print(f\"\\nTotal observations dropped below 0.1th percentile: {total_below}\")\n",
    "print(f\"Remaining dataset shape: {merged_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "56797eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inflation: Dropped 0 above 100.00 (max = 100.0)\n",
      "house_price_change: Dropped 0 above 100.00 (max = 100.0)\n",
      "prob_stocks_up: Dropped 0 above 100.00 (max = 100.0)\n",
      "\n",
      "Total observations dropped above 99.9th percentile: 0\n",
      "Remaining dataset shape: (173446, 33)\n"
     ]
    }
   ],
   "source": [
    "#2.3.2\n",
    "total_above = 0\n",
    "\n",
    "\n",
    "for col in expectation_cols:\n",
    "    q_high = merged_df[col].quantile(0.999)\n",
    "    max_val = merged_df[col].max()\n",
    "\n",
    "    n_above = (merged_df[col] > q_high).sum()\n",
    "    merged_df = merged_df[merged_df[col] <= q_high]\n",
    "\n",
    "    total_above += n_above\n",
    "    print(f\"{col}: Dropped {n_above} above {q_high:.2f} (max = {max_val})\")\n",
    "\n",
    "print(f\"\\nTotal observations dropped above 99.9th percentile: {total_above}\")\n",
    "print(f\"Remaining dataset shape: {merged_df.shape}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TECH2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
